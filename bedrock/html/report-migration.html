<div><span><span>info</span></span></div><div><p><strong>TL;DR:</strong> I took a set of regulated BOXI PDFs and rebuilt them in Power BI Paginated with shared datasets, parameterised templates, and a validation process signed off by Finance. The result is faster change control and audit-ready outputs.</p></div>
<h2>Context</h2>
<p>The business wanted to standardise on a single vendor for both dashboards and flat reports. At the time we used <strong>Power BI</strong> for dashboards and <strong>BusinessObjects (BOXI)</strong> for flat reports. In BOXI, a separate team did the heavy SQL work and exposed a user-friendly drag-and-drop layer. Moving to a SQL-centric workflow in Power BI created an ownership and skills gap. I stepped in, took responsibility for the data modelling and queries, and rewrote every single report.</p>
<p><strong>Goal:</strong> deliver the same Excel/PDF outputs from <strong>Power BI Paginated Reports</strong> that business users previously received from <strong>BOXI</strong>—with tighter governance and quicker change control.</p>
<div><div><div>100+</div><div>Reports Rebuilt</div><div>Over the course of 1 year</div></div><div><div>5</div><div>Critical DW issues fixed</div><div>Issues which impacted our P&#x26;L</div></div><div><div>6x</div><div>Faster response time</div><div>Weeks → Days for complex reports</div></div></div>
<h2>Problem</h2>
<ul>
<li>Inconsistent variable logic across BOXI reports for the same field.</li>
<li>Information asymmetry between the data engineering team and the BI team.</li>
<li>Some BOXI schedules were customer-facing and required a smooth, low-risk transition.</li>
<li>Live BOXI universes existed, but we lacked direct access to query the underlying tables.</li>
<li>Sparse guidance on advanced layouts/expressions for Power BI Paginated (Report Builder).</li>
</ul>
<h2>Approach</h2>
<h3>Creating a checklist</h3>
<p>At the start of each month the data engineering team sent a BOXI Excel export showing all schedules that ran in the previous month. The data was invaluable, but the raw structure made it hard to interpret.</p>
<p>I reshaped it into excels that acted as a point of reference to how far through the migration I was:</p>
<ol>
<li>
<p><strong>Report index (by report name)</strong></p>
<ul>
<li><strong>First tab columns:</strong> <code>report rank</code>, <code>report name</code>, <code># schedules</code>, <code>customer-facing?</code>, <code>Entra ID group?</code></li>
<li>Sort by <code># schedules</code> (desc) and assign a rank. Each <strong>report name</strong> is unique here.</li>
<li>Subsequent tabs follow the pattern <code>report rank | report name</code> and list recipients, destinations, cadence, parameters, and file types.</li>
<li><strong>Outcome:</strong> a prioritised list of high-impact reports and those with customer exposure.</li>
</ul>
</li>
<li>
<p><strong>Recipient index (by business user / group)</strong></p>
<ul>
<li>Group schedules by <strong>recipient</strong> to show total schedules received.</li>
<li>Highlights departments and individuals most reliant on scheduled reports and identifies consolidation opportunities.</li>
</ul>
</li>
</ol>
<h3>Rationalising BOXI logic</h3>
<ul>
<li>
<p><strong>Audit → fix or retire.</strong> For each BOXI report, I reviewed the variable definitions. If a definition was incorrect, I corrected it in the Paginated report. If the report was fundamentally wrong (business rule/logic made the figures meaningless), I <strong>deprecated it</strong> and notified the owning department with evidence and a replacement path where applicable.</p>
</li>
<li>
<p><strong>Canonical metric dictionary.</strong> I normalised calculations into a single semantic SQL layer (CTEs/views) so the same definition is used everywhere (joins, filters, precedence, edge cases).</p>
</li>
<li>
<p><strong>Scalable by design (no more hard coded variables).</strong> Historically, BOXI used separate variables instead of column groups. i.e. If showing sales per warehouse there was a separate variable for warehouse 1/2/3/4. In Power BI Paginated I switched to a <strong>dimension-driven</strong> pattern:</p>
<ul>
<li>Dataset returns one row per <strong>Warehouse</strong>.</li>
<li>The report uses a <strong>Matrix</strong> with a <strong>column group on <code>Warehouse</code></strong> and row groups on the chosen grain.</li>
<li>New warehouses appear automatically at runtime—no new columns, no code changes.</li>
</ul>
</li>
<li>
<p><strong>Example dataset shape (simplified):</strong></p>
<pre><code class="language-sql">SELECT
    dw.warehouse_name,
    fs.PeriodEnd,
    SUM(fs.SalesAmount) AS SalesAmount
FROM fact_sales fs
JOIN dim_warehouse dw ON fs.WarehouseKey = dw.WarehouseKey
JOIN dim_date odate ON odate.dim_date_key = fs.dim_order_date_key
WHERE odate.date BETWEEN @start_date AND @end_date
GROUP BY dw.WarehouseName, f.PeriodEnd;
</code></pre>
</li>
</ul>
<h3>Optimized SQL Queries</h3>
<ul>
<li>A lot of reports would need a dynamic column group that was by year month. If we wanted to show a sales per month for a given year per customer, i.e. 2024 the result set of our SQL query would need to contain a record for every month per customer, like in the below.</li>
</ul>
<p>| customer_number | customer_name | saleperson_name | year | month | cases_sold |
| --- | --- | --- | --- | --- | --- |
| BEN001 | Benjamin's Bakery | Peter Venton | 2024 | 1 | 55 |
| BEN001 | Benjamin's Bakery | Peter Venton | 2024 | 2 | 42 |
| BEN001 | Benjamin's Bakery | Peter Venton | 2024 | 3 | 13 |
| BEN001 | Benjamin's Bakery | Peter Venton | 2024 | 4 | 80 |
| BEN001 | Benjamin's Bakery | Peter Venton | 2024 | 5 | 60 |
| BEN001 | Benjamin's Bakery | Peter Venton | 2024 | 6 | 74 |
| BEN001 | Benjamin's Bakery | Peter Venton | 2024 | 7 | 83 |
| BEN001 | Benjamin's Bakery | Peter Venton | 2024 | 8 | 92 |
| BEN001 | Benjamin's Bakery | Peter Venton | 2024 | 9 | 100 |
| BEN001 | Benjamin's Bakery | Peter Venton | 2024 | 10 | 110 |
| BEN001 | Benjamin's Bakery | Peter Venton | 2024 | 11 | 72 |
| BEN001 | Benjamin's Bakery | Peter Venton | 2024 | 12 | 86 |</p>
<p>There's multiple ways to get this result set but a clear winner in terms of how clean the solution is.</p>
<h4>Anti Patterns</h4>
<ol>
<li><strong>Aggregating with MAX/MIN</strong></li>
</ol>
<p>Works, but every new descriptive field means more aggregates, duplication, and hurts readability. See the query below, if we wanted to add the customer's first address line we'd need to add another <code>MAX()</code> or <code>MIN()</code>.</p>
<pre><code class="language-sql">SELECT
c.customer_number,
MAX(c.customer_name) AS customer_name,
MAX(u.full_name) AS salesperson_name,
odate.year,
odate.month,
SUM(s.cases_sold) AS cases_sold
FROM fact_sales s
JOIN dim_date odate
ON s.dim_order_date_key = odate.dim_date_key
JOIN dim_customer c
ON s.dim_customer_key = c.dim_customer_key
JOIN dim_user u
ON u.dim_user_key = c.dim_salesperson_key
WHERE odate.year = @year --parameter that lets the user pick the year
GROUP BY c.customer_number, odate.year, odate.month
</code></pre>
<ol start="2">
<li><strong>Grouping by every descriptive column</strong></li>
</ol>
<p>Balloons the <code>GROUP BY</code>, makes queries noisy, and can hurt the Query Execution Plan.</p>
<pre><code class="language-sql">SELECT
c.customer_number,
c.customer_name,
u.full_name AS salesperson_name,
odate.year,
odate.month,
SUM(s.cases_sold) AS cases_sold
FROM fact_sales s
JOIN dim_date odate
ON s.dim_order_date_key = odate.dim_date_key
JOIN dim_customer c
ON s.dim_customer_key = c.dim_customer_key
JOIN dim_user u
ON u.dim_user_key = c.dim_salesperson_key
WHERE odate.year = @year --parameter that lets the user pick the year
GROUP BY c.customer_number,
c.customer_name,
u.full_name,
odate.year,
odate.month
</code></pre>
<h4>Clean Pattern</h4>
<ol>
<li><strong>Aggregate the fact</strong> by foreign (dimension) keys and date attributes.</li>
<li><strong>Join dimensions after</strong> to add descriptive columns.</li>
</ol>
<pre><code class="language-sql">WITH cte AS (
  SELECT
  s.dim_customer_key,
  s.dim_salesperson_key,
  odate.year,
  odate.month,
  SUM(s.cases_sold) AS cases_sold
  FROM fact_sales s
  JOIN dim_date odate
  ON odate.dim_date_key = s.dim_order_date_key
  WHERE odate.year = @year --parameter that lets the user pick the year
)

SELECT
c.customer_number,
c.customer_name,
u.full_name AS salesperson_name,
cte.year,
cte.month,
cte.cases_sold
FROM cte
JOIN dim_customer c
ON s.dim_customer_key = cte.dim_customer_key
JOIN dim_user u
ON u.dim_user_key = cte.dim_salesperson_key
</code></pre>
<p>Why this is cleaner:</p>
<ul>
<li>The <code>GROUP BY</code> is short (keys + date), readable, and the QEP will make use of indexes[^1].</li>
</ul>
<p>[^1]: dim_column_keys on the fact are indexed since we know users will use them to join tables.</p>
<ul>
<li>Adding a descriptive field later is trivial (just select another dim_table.column).</li>
</ul>
<h3>Parameterising Reports</h3>
<p>We had a lot of reports which weren't as parameterised as they should be. i.e. A sales report which always showed this month vs last month. Instead of this the powerBI report will:</p>
<ul>
<li>Let the user pick the year and month they want to see (i.e. 2024 - January)</li>
<li>In the SQL work out the previous month</li>
<li>Instead of columns saying this month, last month have them display the year and month, in this case (Sales 2024 - Jan) and (Sales 2023 - Dec)</li>
</ul>
<h3>Validating Reports</h3>
<p>In order to make sure reports matched I would use python + pandas. This allows me to quickly check figures, once I became experienced with how to structure queries / how report builder works I would be able to spend little time on this step.</p>
<pre><code class="language-py">
pbi = pd.read_excel("pbi_report.xlsx", index=["customer_number", "year","month"])
boxi = pd.read_excel("boxi_report.xlsx", index=["customer_number", "year","month"])

# check these are both empty
in_pbi_missing_in_boxi = [rec for rec in pbi.index if rec not in boxi.index]
in_boxi_missing_in_pbi = [rec for rec in boxi.index if rec not in pbi.index]

# if both the above lists are empty then shape should match
assert pbi.shape == boxi.shape

# make sure column names match
col_in_boxi_missing_in_pbi = [rec for rec in pbi if rec not in boxi]
col_in_pbi_missing_in_boxi = [rec for rec in boxi if rec not in pbi]

# if columns are in same order and have different names then easy to update names
boxi_renamings = {old: new for old, new in zip(boxi, pbi)}
boxi = boxi.rename(boxi_renamings, axis=1)

# sort columns, index for pd.compare
pbi = pbi.sort_index(axis=1).sort_index(axis=0)
boxi = boxi.sort_index(axis=1).sort_index(axis=0)

# check if any differences / why they exist
diffs = boxi.compare(pbi, result_names=("boxi", "pbi"))
</code></pre>
<p>I moved this into my custom library, so I wouldn't need to write this logic manually.</p>
<pre><code class="language-py">from mylibrary import check_reports

pbi = pd.read_excel("pbi_report.xlsx", index=["customer_number", "year","month"])
boxi = pd.read_excel("boxi_report.xlsx", index=["customer_number", "year","month"])

result = check_reports(pbi, boxi)

# check result object to see differences, i.e. column names
result.get("column_differences")

# or get the suggested renamings (based on column order)
result.get("column_order_renamings")
</code></pre>
<h3>Scheduling Reports</h3>
<p>BOXI used to have a major issue. We had so many schedules with no way to easily audit them. In order to improve on this I decided to make use of <strong>dynamic per recipient</strong> schedules.</p>
<ul>
<li>BOXI schedules were defined as an instance. On every schedule instance you could add / alter users receiving the schedule. This mean if a new salesperson started someone would have to go into every sales schedule and add the new salesperson onto the schedule.</li>
<li>In PowerBI I setup a <strong>users</strong> semantic model which refreshes daily. The semantic model had no relationships, every table has the same key <code>user_id</code> which contained information for the schedule to be setup. The query for the <strong>dim_salespersons</strong> would look like the below.</li>
</ul>
<pre><code class="language-sql">SELECT
u.user_id AS salesperson_id,
u.user_full_name AS salesperson_full_name,
u.user_email_address AS salesperson_email_address
FROM dim_customer c
JOIN dim_user u
ON u.dim_user_key = c.dim_salesperson_key
WHERE c.is_active_customer = 1
</code></pre>
<ul>
<li>
<p>Then a schedule would be setup to go to every row in this table. It would be sent to every record's <code>salesperson_email_address</code> and the <code>salesperson_id</code> is passed through to the report as a parameter to only show sales of customers for the given salesperson.</p>
</li>
<li>
<p>There are several benefits to this</p>
<ul>
<li>Easy to audit who is receiving schedules</li>
<li>When a new salesperson joins as soon as they are assigned customers in our ERP system they will start receiving reports (no manual intervention needed)</li>
</ul>
</li>
<li>
<p>Some reports have 2-3 dynamic schedules and 1 manual schedule instance with users that are unlikely to change, often these were managers who needed the report but not every manager wanted it which prevented the use of dynamic schedules.</p>
</li>
<li>
<p>For customer facing schedules I made sure to copy the BOXI schedule exactly including the email body. We communicated with customers we're changing our internal tooling but they'd receive the same schedules.</p>
</li>
</ul>
<hr>
<p><strong>Result:</strong> The estate moved from ad-hoc, duplicated logic in BOXI to a governed, reusable Paginated layer on Power BI. Change requests now happen in <strong>days</strong>, not weeks. We can have confidence in reports because the SQL is written within the business from users who understand the business instead of an external team which don't understand the business well enough to build a solution for it (i.e. The BOXI universes).</p>