# Using Python to Syntehsize Data

## Motivation

I found myself wanting to create a portfolio to showcase my ability to use powerBI in an efficient and effective way. The main issue I found was I couldn't find a dataset which I was happy with online, a lot of datasets were messy or limited in what I'd be able to model with them. While I could use power query to create a complex set of steps normally data modelling starts at the data warehouse level not within powerBI. That's when I came up with the idea to synthesize my own data, which I could model in my theoretical data warehouse before pulling into powerBI.

The type of data I wanted to synthesize was my first choice and I decided to synthesize sales data of a company which I could easily expand in the future.


## My Plan
In order to create my data I would use python, this is because of the great tooling python has for data manipulation using [Pandas](https://pandas.pydata.org/) and [Numpy](https://numpy.org/)

The first thing I immediately noted was I would need dimension and fact tables to follow a star schema in powerBI. In terms of dimension tables I knew I needed the following:
- dim_date
- dim_customer
- dim_product
- dim_user

### dim_date
I know I could use the [CALENDAR()](https://learn.microsoft.com/en-us/dax/calendar-function-dax) function to create a calculated table and then compute date features using calculated columns in powerBI, but calculated tables increase a models memory usage and a date table should be stored in the datawarehouse as some features may be tricky to compute in powerBI. As will be shown to work out if a day is a working day we create a holiday table by pulling from an API, something which powerBI does not support natively.

Since I knew dim_date was going to have a record for every calendar date in a selected period I created a dataframe which can be ammended.

```py
dim_date = pd.date_range("2019-01-01", "2030-12-31").to_frame(index=None, name="date")
```

Then I computed useful features, such as month, day of week, etc. I stored this in a function called `get_date_features()` in order to make adding additional features easy in the future.

```py
def get_date_features(df):
    new = df.copy()
    new["dim_date_key"] = new["date"].dt.strftime("%Y%m%d").astype(int)
    new["year"] = new["date"].dt.year
    new["quarter"] = new["date"].dt.quarter
    new["month_num"] = new["date"].dt.month
    new["month_name"] = new["date"].dt.month_name()
    new["day_of_month"] = new["date"].dt.day
    new["day_of_week_num"] = new["date"].dt.dayofweek + 1 
    new["day_of_week_name"] = new["date"].dt.day_name()
    new["is_weekend"] = new["day_of_week_num"].isin([6,7])

    new["day_of_year"] = new["date"].dt.day_of_year
    new["week_of_year"] = new["date"].dt.isocalendar().week
    return new
```

For people not extremely comfortable with pandas you may be wondering why the pattern of df["field"].dt.method has to be followed and not just df["field"].method. Pandas is built on NumPy, which is designed for fast, vectorized operations on arrays. Rather than looping through every element Pandas uses .dt to efficiently apply the datetime operation across the entire column using low level optimizations.

If you have a field which contains python strings in your dataframe and you want to make them lowercase and strip spacing then to do this in python you'd do:
```py
s = " HeLlo World    " #  inconsistent casing and excessive spacing
s = s.strip().lower()
print(s)

>"hello world"
```

If you need to do this to your col in the dataframe then you'd have to do:

```py
df[col] = df[col].str.strip().str.lower()
```

Notice that df[col].str.strip() returns a numpy array and therefore to call the .lower() method on the array you need to once again access the .str property.

Next I wanted to make sure to mark if a day is a trading day. This is because for sales performance you often want to compare it against trading days in the selected period. In order to work out trading days I first had to know if a day was a bank holiday, in order to do this I relied on a public API from the UK Government, https://www.gov.uk/bank-holidays.json. Due to my experience with web scraping and web development dealing with external API's is usually a quick task. If someone is unfamiliar with how to work with an API I'd recommend [Postman](https://www.postman.com/) to understand the idea behind requests, headers, auth tokens, etc...

```py
def get_england_holidays():
    req = requests.get("https://www.gov.uk/bank-holidays.json")
    holidays = json.loads(req.text)
    holidays = holidays["england-and-wales"]["events"]
    df_holidays = pd.DataFrame.from_records(holidays)
    df_holidays["date"] = pd.to_datetime(df_holidays["date"])
    df_holidays = df_holidays.rename({"title": "name_of_holiday"}, axis=1)
    df_holidays = df_holidays[["date", "name_of_holiday"]]
    df_holidays["is_holiday"] = 1
    return df_holidays[["date", "name_of_holiday", "is_holiday"]]
```

I made sure to add the field `df_holidays["is_holiday"] = 1` so once this is merged back onto dim_date there's a flag marking if a day is a holiday.

```py
dim_date = pd.date_range("2019-01-01", "2030-12-31").to_frame(index=None, name="date")
dim_date = get_date_features(dim_date)
df_holidays = get_england_holidays()
dim_date = dim_date.merge(df_holidays, how="left", on="date")
dim_date = fill_missing_date_features(dim_date)
```

In order to fill missing features and have the logic all sit in one place I decided to create a function `fill_missing_date_features()`

```py
def fill_missing_date_features(df):
    new = df.copy()
    new["name_of_holiday"] = new["name_of_holiday"].fillna("")
    new["is_holiday"] = new["is_holiday"].fillna(0).astype(bool)
    return new
```

Finally, I decided to add some working day features which are quick and easy to implement using pandas with groupby() but much harder to implement in powerBI.

```py
def get_working_days_features(df):
    new = df.copy()
    new["is_working_day"] = ~(new["is_weekend"] | new["is_holiday"])
    new["working_day_of_year"] = new.groupby("year")["is_working_day"].transform("cumsum")
    new["working_day_of_quarter"] = new.groupby(["year", "quarter"])["is_working_day"].transform("cumsum")
    new["working_day_of_month"] = new.groupby(["year", "month_num"])["is_working_day"].transform("cumsum")

    return new

dim_date = get_working_days_features(dim_date)
```

While I would love to store this on a Fabric data lake and implement this logic in pyspark, it would cost a lot more than reading from an excel in powerBI, therefore my final step is to export to excel.

```py
dim_date = pd.date_range("2019-01-01", "2030-12-31").to_frame(index=None, name="date")
dim_date = get_date_features(dim_date)
df_holidays = get_england_holidays()
dim_date = dim_date.merge(df_holidays, how="left", on="date")
dim_date = fill_missing_date_features(dim_date)

dim_date = get_working_days_features(dim_date)

dim_date.to_excel("dim_date.xlsx")
```


#### dim_user
I actually went to make dim_product next, but quickly realized the complexity of not having my user table setup. I wanted every product to have a buyer to simulate real world scenarios, often times there will be a stock control team and buyers in the stock control team will be responsible for maintaining the stock levels of different products. Therefore I decided to build a dim_user table, which would have the name of employees and their job titles. Additionally I could use the table to implement RLS in powerBI.

In order to create users I have a few options but I decided to use [Faker](https://fakerjs.dev/), faker is a javascript library for generating dummy data which is available in a variety of languages, however the best place I found for documentation was the javascript docs https://fakerjs.dev/.

The first step to a user table is to generate first and last names. In order to do this I had a `generate_name_df()` function which took the number of names to generate as an argument. Since I wanted every user to be unique I kept track of all names encountered so far using a set and checking every name was new per loop iteration.

```py
def generate_name_df(n: int):
    fake = Faker("en-GB")
    names = set()
    while len(names) < n:
        name = (fake.first_name(), fake.last_name())
        if name not in names:
            names.add(name)

    name_df = pd.DataFrame.from_records(list(names), columns=["user_first_name", "user_last_name"])

    return name_df
```

Next I wanted to have every user have a unique code. It would be their first and last intial with a 3 digit number starting from 001, if a user with the same code already existed the number would increment by 1. To do this I decided to use the itertuples function, this may not be the cleanest bit of code and in a production environment I would write a function that takes a users full name and returns a dict of `user_full_name: user_code` but for how small the project is this was the solution I settled for.

The function I created is `get_user_codes()`, it iterates over the dataframe and gets the first initial, last inital and upper cases them, then I have a dictionary to keep track of how many times the initials were already seen. You will see below `num = seen[code]`, if I defined seen as a normal dictionary I'd have to do `num = seen.setdefault(code, 1)` however I decided to use a [defaultdict](https://docs.python.org/3/library/collections.html#collections.defaultdict) for readabilty. Additionally, my function is "private" in the sense that I will create another function called `get_user_features()` which will use this function to update the user features.

```py
def _get_user_codes(df: pd.DataFrame):
    new = df.copy()
    seen = defaultdict(lambda: 1)
    mappings = {}

    for idx, first, last in new.itertuples():
        code = first[0].upper() + last[0].upper()
        num = seen[code]
        seen[code] = num + 1
        mappings[idx] = f"{code}{num:03}"

    new["user_code"] = new.index.map(mappings)

    return new
```


Finally I wanted to be able to split my users into different departments. Therefore I created a function that took a dictionary of job roles and the number of jobs to create. The dictionary of job roles looked as follows:

```py
# values must add to 1
JOB_ROLES = {
    "external_salesperson": 0.1,
    "internal_salesperson": 0.1,
    "buyer": 0.1,
    "financial_controller": 0.1,
    "stock controller (fast moving line)": 0.1,
    "stock controller (medium moving line)": 0.1,
    "stock controller (slow moving line)": 0.1,
    "stock controller (bespoke)": 0.1,
    "purchasing": 0.1,
    "marketing": 0.025,
    "website": 0.025,
    "commercial": 0.05
}
```

i.e. If I pass n=10 to _generate_job_roles() then 1 item in the list would be an external_salesperson. However I cant have half an item be commercial or a quarter of an item be marketing, therefore there was a check to assure n*dict_value is an integer. Additionally, there is a check to make sure values in the dict add to 1.

```py
def _generate_job_roles(job_roles, n):
    final_roles = []
    if sum(job_roles.values()) != 1:
        raise ValueError(f"Job roles passed into `generate_roles()` adds to {sum(job_roles.values())}. Must add to 1")
    
    for job, pct in job_roles.items():
        num_users_for_job = pct * n # number of users that will be assigned to that job role
        if not num_users_for_job.is_integer():
            raise ValueError(f"bad args passed to generate_roles(), {job=} has a percentage specified of {pct=}, asking for {n=} total rows to be returned but {pct}*{n}={pct*n:.02} which isn't an integer")
        final_roles.extend([job] * int(num_users_for_job))

    np.random.shuffle(final_roles)

    return final_roles
```

You will see in the above 
```py
raise ValueError(f"bad args passed to generate_roles(), {job=} has a percentage specified of {pct=}, asking for {n=} total rows to be returned but {pct}*{n}={pct*n:.02} which isn't an integer")
```

The syntax of [`var=`](https://docs.python.org/3/tutorial/inputoutput.html#formatted-string-literals) is new as of python 3.8 but it makes creating readable error messages very quick.

Finally I create a get_user_features() which wraps the 2 above functions and adds a user_full_name and a user_email_address to make the code easy to read.

```py
def get_user_features(df):
    new = df.copy()
    new = _get_user_codes(new) # has to be 2nd bc using itertuples

    new["user_full_name"] = new["user_first_name"] + " " + new["user_last_name"] 
    new["user_email_address"] = (new["user_first_name"] + "." + new["user_last_name"] + "@apertureanalytics.co.uk").str.lower()
    new["user_job_role"] = _generate_job_roles(JOB_ROLES, N)

    return new
```

The main function is then
```py
fake = Faker("en-GB")

N = 200 # users to generate


dim_user = generate_name_df(N)

dim_user = get_user_features(dim_user)

dim_user.to_excel("dim_user.xlsx")
```

#### dim_product

dim_product was a lot trickier to create than I first thought. The issue was getting products with names that made sense and categories/prices that matched the name. i.e. a Large cup should be more expensive than a small cup. Due to me needing this data to power a PowerBI portfolio I decided the easiest way to get the data would be to get it externally instead of synthesizing it.

Initially I looked at online datasets but this felt like a cheat, after all this project was meant to show my python skills. Therefore I decided to pull data from an online page, which will remain anonymous. Due to my knowledge of web development I know how to scrape a page in a non-intrusive / low impace way. Additionally it would be very clear based on my request that I was not a user accessing the site from a browser. The easiest way to know this is my http requests deliberately did not have headers set, browsers send over hundreds of headers per request the most common being user-agent which tells the website the device you're making the request from. The absence of these headers is a clear indicator the request is not coming from a user on a browser.

In order to scrape data there are a few options, a lot of sites nowadays are [SPAs](https://en.wikipedia.org/wiki/Single-page_application) meaning you'd need to load the javascript to [hydrate](https://en.wikipedia.org/wiki/Hydration_(web_development)) the html content of the page. In order to do this you'd need a headless browser and to use a tool like [Selenium](https://www.selenium.dev/) or a framework like [Scrapy](https://www.scrapy.org/) that could run the browser in a [headless](https://developer.chrome.com/docs/chromium/headless) mode to pull the html content. This was something I avoided and made sure to find a site where I could use the python requests library and not need to over engineer a web scraper for such a simple task of populating my dim_product table.

In order to parse the returned html I decided to use [Beautiful Soup (bs4)](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), this allowed me to parse the html tree for specific elements.

In order to make sure I didn't send too many requests I made sure to create a function called `wait_and_get()` which wrapped `requests.get()`

```py
def wait_and_get(url):
    time.sleep(15)
    return requests.get(url)
```

Sleeping for 15 seconds is quite generous, after looking at the site's [robots.txt](https://en.wikipedia.org/wiki/Robots.txt) I saw they were asking for a 10 second delay between requests for web crawlers.

To start off I had to navigate the site to get to a product catalogue.

```py
def get_category_links():
    site = "https://www.***.com/"

    r = wait_and_get(site)

    soup = BeautifulSoup(r.text, "html.parser")

    return set([el.get("href") for el in soup.find_all("a", class_="categories-image")])  # has duplicate links to make the site seem more full
```

Once I was on a catalogue page I had a function which would go through every product on the catalogue and for every page. Then it would call a function to get the product attributes, the helper function took a bs4 tag, which is just a subsection of the page of the html just for the product

```py
def get_product_page_info(soup):
    final = []
    page_numbers = soup.find("ul", class_="page-numbers")

    if page_numbers:
        page_urls = [a.get("href") for a in page_numbers.find_all("a") if page_numbers] # current page doesn't have a url use this as a stack
    else:
        page_urls = []
    while True:
        products = soup.find_all("li", class_="product")

        for product in products:
            final.append(get_product_attributes(product))

        if not page_urls:
            break
        
        new_url = page_urls.pop(0)
        r = wait_and_get(new_url)
        soup = BeautifulSoup(r.text, "html.parser")
    
    return final
```
To get the product attributes I had a function extract the product_code, product_description, a list of product_categories, the product's price including VAT tax and the products price excluding VAT tax.

I ran into the issue that a product didn't have a price listed therefore as a quick fix I wrapped the logic which extracts the price in a try, except incase it returned None. (In the end this only affected 1 product for which I would generate a random price as seen later)

```py
def get_product_attributes(product_soup: Tag):
    class_string = " ".join(product_soup.get("class"))
    product_categories = [s.replace("-", " ").capitalize() for s in re.findall(r"product_cat-(\S+)", class_string)]
    
    try:
        price_tag = product_soup.find("span", class_="price").get_text(strip=True)
        price_excl_vat, price_incl_vat = re.match(r"£(\d+\.\d+)Ex VAT \(£(\d+\.\d+)Inc VAT\)", price_tag).groups()
    except:
        print('couldnt find price', product_soup.prettify())
        price_excl_vat, price_incl_vat = 0, 0
        
    product_title = product_soup.find("h2", class_="woocommerce-loop-product__title").get_text(strip=True)

    product_code = product_soup.find("a", attrs={"data-product_sku": True}).get("data-product_sku")
    return product_code,product_title, product_categories, float(price_incl_vat), float(price_excl_vat)    
```

The main body which glued all these functions together was more complex than I'd like it to be but it got me the information I needed. I saved the final list of records as a .pkl file. [Pickle](https://docs.python.org/3/library/pickle.html) is a module in the standard library which provides way to serialize and unsereliaze python objects.

```py
links = get_category_links()

product_records_df = []
visited = set()
for link in links:
    queue = [link]

    while queue:
        curr = queue.pop(0)
        if curr in visited:
            continue
        r = wait_and_get(curr)
        visited.add(curr)
        soup = BeautifulSoup(r.text, "html.parser")
        products = soup.find("ul", class_="products")

        if products.find("li", class_="product-category"):
            hrefs = [a.get("href") for a in products.find_all("a")]
            queue.extend(hrefs)
        else:
            product_records_df.extend(get_product_page_info(soup))

with open("records.pkl", "wb") as f:
    pickle.dump(product_records_df, f)
```


Once I had my "records.pkl" I made sure to assign buyers to products.

```py
import pickle
import pandas as pd
import numpy as np
import re

def get_product_family(product_code):
    r = re.match(r"^([^\d]*)", product_code)
    return r.groups()[0]

def map_buyers_to_products(buyers, product_codes):
    np.random.shuffle(product_codes)
    
    buyer_to_product_ratio = len(product_codes) // len(buyers)
    buyer_to_product_remainder = len(product_codes) % len(buyers)

    final_mappings = {}
    index = 0

    for buyer in buyers:

        extra = 1 if buyer_to_product_remainder > 0 else 0
        count = buyer_to_product_ratio + extra

        buyer_products = product_codes[index:index + count]
        index += count

        if buyer_to_product_remainder > 0:
            buyer_to_product_remainder -= 1

        final_mappings.update({p: buyer for p in buyer_products})

    return final_mappings


users = pd.read_excel("dim_user.xlsx", index_col=0)
buyers = users.loc[users["user_job_role"] == "buyer", "user_code"]  # every user is unique

with open("records.pkl", "rb") as f:
    products = pickle.load(f)

df = pd.DataFrame.from_records(products, columns=["product_code","product_description", "categories", "price_excl_vat", "price_incl_vat"])

dim_product = df.drop("categories", axis=1)

dim_product["product_family"] = dim_product["product_code"].apply(get_product_family)

product_families = dim_product["product_family"].unique()

dim_product["buyer"] = dim_product["product_family"].map(map_buyers_to_products(buyers,product_families))

dim_product.to_excel("dim_product.xlsx")
```

### dim_customer